# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Nx6FGZkU0ntYyYsRslgdJuFF-uPeyiH9

1. Load and Explore the Data (EDA)
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Update file_path to the absolute path
file_path = '/carros.csv'

try:
    df = pd.read_csv(file_path)
except FileNotFoundError:
    print(f"Error: File not found at '{file_path}'. Please check the file path and ensure the file exists.")
    # You can add logic here to handle the error, like prompting the user for the correct path
    # or exiting the script. For example:
    # file_path = input("Enter the correct file path: ")
    # df = pd.read_csv(file_path)
    # ...
    # Handle the error. For example, you could raise the exception or exit the script.
    # Here, we'll just re-raise the exception to stop execution:
    raise  # This line will re-raise the FileNotFoundError

# Display basic info and statistics
df.info()
df.describe()

# Visualize relationships
# Adjust the columns used in pairplot and other visualizations according to the columns in carros.csv
sns.pairplot(df[['selling_price', 'km_driven', 'fuel', 'seller_type', 'transmission', 'owner']]) # Replace with relevant columns from carros.csv
plt.show()

# Identify outliers
sns.boxplot(x='selling_price', data=df)  # Replace with relevant column from carros.csv
plt.show()

# Explore trends
df.groupby('year')['selling_price'].mean().plot() # Replace with relevant columns from carros.csv
plt.show()

# ... (Add more visualizations as needed) ...

"""2. Data Preprocessing"""

# Handle missing values
# Create a list to store columns that were successfully converted to numeric
numeric_columns = []

# Convert numeric columns to numeric type if they are not
for column in df.select_dtypes(include=['object']).columns:
    try:
        # Attempt to convert to numeric, handling potential errors
        df[column] = pd.to_numeric(df[column], errors='raise')
        numeric_columns.append(column)  # Add to the list if successful
    except ValueError:
        # If conversion fails, print a warning and skip the column
        print(f"Column '{column}' could not be converted to numeric. Skipping for imputation.")

# Create a copy of the DataFrame with only numeric columns for imputation
numeric_df = df[numeric_columns].copy()  # Explicitly create a copy

# Fill missing values for numerical features with their mean
numeric_df.fillna(numeric_df.mean(), inplace=True)

# Update the original DataFrame with imputed values
df.update(numeric_df) # Update the original df with the filled values

"""3. Feature Selection"""

import pandas as pd
from sklearn.feature_selection import SelectKBest, f_regression

# ... (your previous code, including handling missing values) ...

# Check if columns related to fuel type exist
fuel_columns = [col for col in df.columns if 'fuel' in col]

# Modified condition: Check if fuel_columns is not empty
if fuel_columns:
    # If fuel-related columns exist, proceed with one-hot encoding
    df = pd.get_dummies(df, columns=fuel_columns, prefix=['fuel'], drop_first=False)
else:
    # Handle the case where no fuel-related columns are found
    # Instead of raising KeyError, print a warning or handle it differently
    print("Warning: No columns related to 'fuel' were found. Skipping one-hot encoding for fuel.")
    # You can add alternative logic here if needed, such as:
    # - Creating a new 'fuel' column with a default value
    # - Skipping this step and proceeding with other parts of your analysis

"""4. Train-Test Split"""

import pandas as pd
from sklearn.feature_selection import SelectKBest, f_regression
from sklearn.model_selection import train_test_split


# Assuming 'df' is your DataFrame after preprocessing
# Select features (X) and target (y)
# Replace 'selling_price' with the actual name of your target variable column
y = df['selling_price']  # Changed 'target_variable_name' to 'selling_price'
X = df.drop(columns=['selling_price']) # Features are everything except the target

# ... (your previous code, including handling missing values and one-hot encoding) ...

# Now assign the processed features to X_new
X_new = X  # Or X_new = df[selected_features] if you have specific features selected

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_new, y, test_size=0.2, random_state=42)

"""5. Model Training and Hyperparameter Tuning"""

import pandas as pd
from sklearn.feature_selection import SelectKBest, f_regression
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import GridSearchCV
from sklearn.impute import SimpleImputer  # Import SimpleImputer for imputation

# ... (your previous code, including handling missing values and one-hot encoding) ...

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_new, y, test_size=0.2, random_state=42)

# Impute missing values in X_train and X_test using SimpleImputer
imputer = SimpleImputer(strategy='mean')  # or 'median', 'most_frequent'
X_train = imputer.fit_transform(X_train)  # Fit and transform on training data
X_test = imputer.transform(X_test)      # Transform testing data using fitted imputer


# Define model and hyperparameter grid
model = LinearRegression()
param_grid = {}  # Add hyperparameters if needed (e.g., for regularization)

# Use GridSearchCV for hyperparameter tuning
grid_search = GridSearchCV(model, param_grid, cv=5, scoring='neg_mean_squared_error')
grid_search.fit(X_train, y_train)

# Get the best model
best_model = grid_search.best_estimator_

"""6. Model Evaluation


"""

from sklearn.metrics import mean_squared_error, r2_score

y_pred = best_model.predict(X_test)
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
print("Mean Squared Error:", mse)
print("R-squared:", r2)

# ... (Calculate other metrics like precision, recall, F1-score if applicable) ...



"""7. Visualization"""

plt.scatter(y_test, y_pred)
plt.xlabel("Actual Prices")
plt.ylabel("Predicted Prices")
plt.title("Actual vs. Predicted Prices")
plt.show()

# ... (Create other visualizations like residual plots, feature importance plots, etc.) ...

"""vino tinto

1. Análisis Exploratorio de Datos (EDA)
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Cargar el dataset
df = pd.read_csv('/content/vino tinto.csv')

# Mostrar información básica
df.info()
df.describe()

# Visualizar relaciones entre variables
sns.pairplot(df, hue='quality')  # 'quality' como variable de color
plt.show()

# Identificar valores atípicos con boxplots
plt.figure(figsize=(12, 6))
sns.boxplot(data=df, orient='h')
plt.title('Boxplots para detectar valores atípicos')
plt.show()

# Explorar tendencias con gráficos de línea
df.groupby('alcohol')['quality'].mean().plot(kind='line')
plt.title('Tendencia de la calidad en función del alcohol')
plt.xlabel('Alcohol')
plt.ylabel('Calidad promedio')
plt.show()

# Correlación entre variables
plt.figure(figsize=(10, 8))
sns.heatmap(df.corr(), annot=True, cmap='coolwarm')
plt.title('Matriz de correlación')
plt.show()

"""2. Preprocesamiento de Datos"""

# Manejo de valores faltantes (si los hay)
# Imputar con la media para variables numéricas
for col in df.select_dtypes(include=['number']).columns:
    df[col] = df[col].fillna(df[col].mean())

# Transformar variables categóricas (si las hay)
# df = pd.get_dummies(df, columns=['variable_categorica'], drop_first=True)

# Escalar las características (opcional pero recomendado)
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
numerical_features = df.select_dtypes(include=['number']).columns
df[numerical_features] = scaler.fit_transform(df[numerical_features])

"""3. Selección de Características"""

from sklearn.feature_selection import SelectKBest, f_classif

# Seleccionar las mejores características (k=5 como ejemplo)
X = df.drop(columns=['quality'])  # Variables predictoras
y = df['quality']  # Variable objetivo
selector = SelectKBest(f_classif, k=5)
X_new = selector.fit_transform(X, y)

# Obtener los nombres de las características seleccionadas
selected_features = X.columns[selector.get_support()]
print("Características seleccionadas:", selected_features)

"""4. División Entrenamiento-Prueba"""

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X_new, y, test_size=0.2, random_state=42)

"""5. Entrenamiento y Ajuste de Hiperparámetros del Modelo"""

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV

# Definir el modelo y la grilla de hiperparámetros
model = LogisticRegression()
param_grid = {'C': [0.1, 1, 10]}

"""6. Modelo y visualizacion"""

# 5. Entrenamiento del Modelo y Ajuste de Hiperparámetros
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV
from sklearn.preprocessing import LabelBinarizer # Import LabelBinarizer if y has more than 2 unique values

# If your target variable 'y' is continuous and you intend to perform classification:
# 1. Convert 'y' to discrete classes. You may need to define thresholds or use methods like binning.
# For example, if 'y' represents wine quality, you could categorize it into 'good' and 'bad' based on a threshold.
# Assume 'y' should be 1 if wine quality is above 6 and 0 otherwise:
# Adjust the threshold (e.g., to the median or mean) to ensure you have both classes
threshold = y_train.median()  # Or y_train.mean()
y_train_classified = (y_train > threshold).astype(int)
y_test_classified = (y_test > threshold).astype(int)

# Or, if 'y' has multiple unique values > 2 and you need one-hot encoding:
# lb = LabelBinarizer()
# y_train_classified = lb.fit_transform(y_train)
# y_test_classified = lb.transform(y_test)

# Definir el modelo y la grilla de hiperparámetros
model = LogisticRegression(solver='liblinear', max_iter=1000)
param_grid = {'C': [0.1, 1, 10], 'penalty': ['l1', 'l2']}

# Búsqueda de cuadrícula para encontrar los mejores hiperparámetros
grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy', n_jobs=-1)
# Fit using the classified target variable
grid_search.fit(X_train, y_train_classified)

# ... (rest of the code remains the same, but use y_test_classified for predictions)
y_pred = grid_search.predict(X_test) # predict using the original X_test # Changed from best_model to grid_search

# ... (Evaluation metrics and visualization should be adjusted for
# classification tasks)

"""1. ROC Curve and AUC Score:"""

!pip install matplotlib scikit-learn

import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, roc_auc_score

# Assuming you have already trained the model and have y_test_classified and y_pred_proba
y_pred_proba = grid_search.predict_proba(X_test)[:, 1]  # Probability of class 1

fpr, tpr, thresholds = roc_curve(y_test_classified, y_pred_proba)
auc_score = roc_auc_score(y_test_classified, y_pred_proba)

plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {auc_score:.2f})')
plt.plot([0, 1], [0, 1], 'k--')  # Diagonal line representing random classifier
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc='lower right')
plt.show()

"""2. Precision-Recall Curve:"""

!pip install matplotlib scikit-learn

import matplotlib.pyplot as plt
from sklearn.metrics import precision_recall_curve, average_precision_score

# Assuming you have already trained the model and have y_test_classified and y_pred_proba
y_pred_proba = grid_search.predict_proba(X_test)[:, 1]  # Probability of class 1

precision, recall, thresholds = precision_recall_curve(y_test_classified, y_pred_proba)
avg_precision = average_precision_score(y_test_classified, y_pred_proba)

plt.figure(figsize=(8, 6))
plt.plot(recall, precision, label=f'Precision-Recall Curve (AP = {avg_precision:.2f})')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve')
plt.legend(loc='lower left')
plt.show()

"""3. Confusion Matrix:"""

!pip install matplotlib scikit-learn

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix

# Assuming you have already trained the model and have y_test_classified and y_pred
cm = confusion_matrix(y_test_classified, y_pred)

plt.figure(figsize=(6, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix')
plt.show()

"""Corazon"""

# Importar las bibliotecas necesarias
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

# 1. Análisis exploratorio de datos
# Cargar el dataset
df = pd.read_csv('/content/corazon.csv')

# Imprimir los nombres de las columnas para verificar el nombre correcto de la columna objetivo
print(df.columns)

# Visualizar las primeras filas
print("Primeras filas del dataset:")
print(df.head())

# Información general del dataset
print("\nInformación del dataset:")
print(df.info())

# Resumen estadístico
print("\nResumen estadístico:")
print(df.describe())

# Verificar valores faltantes
print("\nValores faltantes por columna:")
print(df.isnull().sum())

# Matriz de correlación
plt.figure(figsize=(12, 8))
sns.heatmap(df.corr(), annot=True, cmap='coolwarm')
plt.title("Matriz de correlación")
plt.show()

# Boxplots para identificar valores atípicos
for column in df.select_dtypes(include=np.number).columns:
    plt.figure(figsize=(8, 4))
    sns.boxplot(data=df, x=column)
    plt.title(f"Boxplot de {column}")
    plt.show()

# 2. Preprocesamiento de los datos
# Imputación de valores faltantes (si los hay)
df.fillna(df.median(), inplace=True)

# Verificar nuevamente valores faltantes
print("\nValores faltantes después del preprocesamiento:")
print(df.isnull().sum())

# Codificación de variables categóricas (si las hay)
df = pd.get_dummies(df, drop_first=True)

# 3. Selección de características
# Separar características y la variable objetivo
# Obtener el nombre de la columna objetivo de df.columns (asumiendo que es la última columna)
target_column = df.columns[-1]
# O si conoces el nombre exacto, asígnalo directamente:
# target_column = 'nombre_correcto_de_la_columna_objetivo'
X = df.drop(columns=[target_column])
y = df[target_column]

# Selección de características con Random Forest
model_rf = RandomForestClassifier(random_state=42)
model_rf.fit(X, y)
importances = pd.DataFrame({'Feature': X.columns, 'Importance': model_rf.feature_importances_})
importances = importances.sort_values(by='Importance', ascending=False)

# Visualizar las características más importantes
plt.figure(figsize=(10, 6))
sns.barplot(data=importances, x='Importance', y='Feature')
plt.title("Importancia de las características")
plt.show()

# Usar solo las características más importantes (opcional)
selected_features = importances['Feature'].head(10).values
X = X[selected_features]

# 4. Dividir el dataset en Train y Test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# 5. Entrenamiento del modelo
# Configurar hiperparámetros del modelo
model = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)
model.fit(X_train, y_train)

# 6. Evaluación del modelo
# Predicciones en el conjunto de test
y_pred = model.predict(X_test)

# Métricas de evaluación
print("\nReporte de clasificación:")
print(classification_report(y_test, y_pred))

print("\nMatriz de confusión:")
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title("Matriz de confusión")
plt.xlabel("Predicción")
plt.ylabel("Valor real")
plt.show()

# Precisión global
accuracy = accuracy_score(y_test, y_pred) # Assign the result of accuracy_score to accuracy